# Using library Python OpenCL for split task with Ollama GPUs
Code generate with AI ( is not tested) Be carefull


Q : What does this code ?

R : Split the task between GPU internal 

Example  GPU + IGPU 
        GPu = Nvidia or ATI Radeon
        CPU graphic card = Intel graphic Card or AMD ati 


#################################################################################
#################################################################################

This is split by two in your graphics card and don't touch the space of kernel or OS.
IN other words this code don't whant to touch ram from Computer.

<------------MODELS----------------------------
For better performance i suggest to you to use SLM or use LLM using different "Q"
Small  language model : https://en.wikipedia.org/wiki/Small_language_model
Large  language model : https://en.wikipedia.org/wiki/Large_language_model
"Medium" language model : https://arxiv.org/abs/2305.11991


#################################################################################
#################################################################################
EXpand ??? maybe creating a clustering but it's complicated i think.... 
